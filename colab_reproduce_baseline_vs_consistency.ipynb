{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## VideoMetamaterials: 复现原始实验 + 加入 multi-scale consistency loss 对比（Colab）\n",
        "\n",
        "本 notebook 用于在 **Google Colab (单卡 GPU)** 上：\n",
        "\n",
        "- **下载并使用论文提供的同一数据集与预训练 checkpoint**（来自 Zenodo / ETHZ Research Collection）\n",
        "- **复现 baseline**（`lambda_phys = 0`）：加载预训练模型，按论文方式生成条件采样视频\n",
        "- **加入 physics consistency loss**（`lambda_phys > 0`）：在相同数据集上继续训练（fine-tune 或从头训练），并生成同样的条件采样视频\n",
        "- **对比评估**：\n",
        "  - 多尺度自洽指标（RG-style）：对生成视频做空间 coarse-graining，比较各尺度的统计表征差异\n",
        "  - 可视化对比：输出 GIF 网格与 topologies\n",
        "\n",
        "> 注意：论文的最终“物理真实性”评估需要 Abaqus 仿真。Colab 环境不包含 Abaqus，因此本 notebook 的对比侧重于 **生成质量 + 多尺度自洽性**（即你引入的 consistency loss 的直接目标）。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 0) 环境检查 ===\n",
        "import os, sys, json, textwrap, subprocess, re\n",
        "import torch\n",
        "\n",
        "print('python:', sys.version)\n",
        "print('torch:', torch.__version__)\n",
        "print('cuda available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('gpu:', torch.cuda.get_device_name(0))\n",
        "    print('capability:', torch.cuda.get_device_capability(0))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 1) 安装依赖（按 README 的最小集合）===\n",
        "# Colab 自带 torch，通常无需降级；其余按需安装\n",
        "!pip -q install einops==0.6.1 einops-exts==0.0.4 rotary-embedding-torch==0.2.3 accelerate==0.19.0 imageio==2.28.1 tqdm==4.65.0 networkx==2.8.4 matplotlib==3.8.0 pillow==10.0.1 pyyaml\n",
        "\n",
        "# 可选：wandb\n",
        "# !pip -q install wandb==0.15.2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 2) 进入项目目录（不使用 notebook magic，保证可复现）===\n",
        "import subprocess, pathlib\n",
        "\n",
        "REPO_DIR = '/content/VideoMetamaterials-main'\n",
        "\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    subprocess.run(['git', 'clone', '--quiet', 'https://github.com/jhbastek/VideoMetamaterials.git'], check=True)\n",
        "    # 原仓库目录一般叫 /content/VideoMetamaterials\n",
        "    if os.path.exists('/content/VideoMetamaterials') and not os.path.exists(REPO_DIR):\n",
        "        pathlib.Path(REPO_DIR).symlink_to('/content/VideoMetamaterials')\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "print('cwd:', os.getcwd())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3) 下载同一数据集与预训练模型（Zenodo API 自动下载）\n",
        "\n",
        "论文作者在 README 里给了数据与 checkpoint 的下载入口（ETHZ Research Collection / Zenodo）。这里用 **Zenodo API** 自动拉取并解压到正确目录结构：\n",
        "\n",
        "- `data/lagrangian.zip` → 解压到 `data/lagrangian/`\n",
        "- `runs/pretrained.zip` → 解压到 `runs/pretrained/`\n",
        "\n",
        "如果网络无法访问 Zenodo，你也可以在 Colab 左侧手动上传 `lagrangian.zip` 和 `pretrained.zip`，然后运行同一段解压代码。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests, zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "ZENODO_RECORD_ID = 10011767  # 来自 README 的 Zenodo DOI: 10.5281/zenodo.10011767\n",
        "\n",
        "DATA_ZIP = Path('data/lagrangian.zip')\n",
        "RUNS_ZIP = Path('runs/pretrained.zip')\n",
        "\n",
        "DATA_ZIP.parent.mkdir(parents=True, exist_ok=True)\n",
        "RUNS_ZIP.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "def zenodo_download(record_id: int, filename: str, out_path: Path):\n",
        "    url = f'https://zenodo.org/api/records/{record_id}'\n",
        "    r = requests.get(url, timeout=60)\n",
        "    r.raise_for_status()\n",
        "    j = r.json()\n",
        "    files = j.get('files', [])\n",
        "    match = None\n",
        "    for f in files:\n",
        "        if f.get('key') == filename:\n",
        "            match = f\n",
        "            break\n",
        "    if match is None:\n",
        "        raise RuntimeError(f'Zenodo record {record_id} does not contain {filename}. Found: {[f.get(\"key\") for f in files]}')\n",
        "\n",
        "    dl_url = match['links']['self']  # direct file endpoint\n",
        "    print('Downloading:', dl_url)\n",
        "    with requests.get(dl_url, stream=True, timeout=60) as resp:\n",
        "        resp.raise_for_status()\n",
        "        total = int(resp.headers.get('content-length', 0))\n",
        "        chunk = 1024 * 1024\n",
        "        out_path_tmp = out_path.with_suffix(out_path.suffix + '.part')\n",
        "        written = 0\n",
        "        with open(out_path_tmp, 'wb') as f:\n",
        "            for b in resp.iter_content(chunk_size=chunk):\n",
        "                if not b:\n",
        "                    continue\n",
        "                f.write(b)\n",
        "                written += len(b)\n",
        "                if total:\n",
        "                    print(f'  {written/total:.1%}', end='\\r')\n",
        "        out_path_tmp.replace(out_path)\n",
        "    print('Saved:', out_path)\n",
        "\n",
        "\n",
        "def unzip_to(zip_path: Path, target_dir: Path):\n",
        "    print('Unzipping', zip_path, '->', target_dir)\n",
        "    target_dir.mkdir(parents=True, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "        z.extractall(target_dir)\n",
        "\n",
        "\n",
        "# 自动下载（如文件不存在）\n",
        "try:\n",
        "    if not DATA_ZIP.exists():\n",
        "        zenodo_download(ZENODO_RECORD_ID, 'lagrangian.zip', DATA_ZIP)\n",
        "    if not RUNS_ZIP.exists():\n",
        "        zenodo_download(ZENODO_RECORD_ID, 'pretrained.zip', RUNS_ZIP)\n",
        "except Exception as e:\n",
        "    print('自动下载失败：', repr(e))\n",
        "    print('请在 Colab 左侧手动上传 lagrangian.zip 与 pretrained.zip 到 data/ 与 runs/ 目录，再继续运行下面的解压单元。')\n",
        "\n",
        "\n",
        "# 解压到正确位置\n",
        "if DATA_ZIP.exists() and not Path('data/lagrangian').exists():\n",
        "    unzip_to(DATA_ZIP, Path('data'))\n",
        "\n",
        "if RUNS_ZIP.exists() and not Path('runs/pretrained').exists():\n",
        "    unzip_to(RUNS_ZIP, Path('runs'))\n",
        "\n",
        "print('data/ contains:', sorted([p.name for p in Path('data').iterdir()]))\n",
        "print('runs/ contains:', sorted([p.name for p in Path('runs').iterdir()]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4) Baseline：加载论文预训练模型并生成条件采样视频\n",
        "\n",
        "这一步复现 README 的行为：用 `data/target_responses.csv` 里的目标应力-应变曲线做 conditioning，生成视频并保存到 `runs/pretrained/eval_target_w_<w>_*/step_<step>/`。\n",
        "\n",
        "为避免 Colab 下分布式初始化的问题，我们不调用仓库里的 `main.py`，而是在 notebook 里用同样的方式构建 `Unet3D / GaussianDiffusion / Trainer` 并加载 checkpoint。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 4.1) 加载配置 & 构建模型/Trainer（单卡 Colab，无需 torch.distributed）===\n",
        "import yaml\n",
        "from accelerate import Accelerator\n",
        "\n",
        "from denoising_diffusion_pytorch import Unet3D, GaussianDiffusion, Trainer\n",
        "\n",
        "\n",
        "def load_yaml(path: str):\n",
        "    with open(path, 'r') as f:\n",
        "        return yaml.safe_load(f)\n",
        "\n",
        "\n",
        "def build_accelerator(fp16: bool = True):\n",
        "    return Accelerator(mixed_precision='fp16' if fp16 else 'no')\n",
        "\n",
        "\n",
        "def build_model_and_trainer(config: dict, run_dir: str, log: bool = False, fp16: bool = True):\n",
        "    accelerator = build_accelerator(fp16=fp16)\n",
        "\n",
        "    model = Unet3D(\n",
        "        dim = config['unet_dim'],\n",
        "        dim_mults = (1, 2, 4, 8),\n",
        "        channels = len(config['selected_channels']),\n",
        "        attn_heads = config['unet_attn_heads'],\n",
        "        attn_dim_head = config['unet_attn_dim_head'],\n",
        "        init_dim = None,\n",
        "        init_kernel_size = 7,\n",
        "        use_sparse_linear_attn = config['unet_use_sparse_linear_attn'],\n",
        "        resnet_groups = config['unet_resnet_groups'],\n",
        "        cond_bias = True,\n",
        "        cond_attention = config['unet_cond_attention'],\n",
        "        cond_attention_tokens = config['unet_cond_attention_tokens'],\n",
        "        cond_att_GRU = config['unet_cond_att_GRU'],\n",
        "        use_temporal_attention_cond = config['unet_temporal_att_cond'],\n",
        "        cond_to_time = config['unet_cond_to_time'],\n",
        "        per_frame_cond = config['per_frame_cond'],\n",
        "        padding_mode = config['padding_mode'],\n",
        "    )\n",
        "\n",
        "    diffusion = GaussianDiffusion(\n",
        "        model,\n",
        "        image_size = 96,\n",
        "        channels = len(config['selected_channels']),\n",
        "        num_frames = 11,\n",
        "        timesteps = config['train_timesteps'],\n",
        "        loss_type = 'l1',\n",
        "        use_dynamic_thres = config['use_dynamic_thres'],\n",
        "        sampling_timesteps = config['sampling_timesteps'],\n",
        "        # physics consistency (我们在 fork 版里加的参数；原版没有也不会报错，因为我们只在本 repo 使用)\n",
        "        lambda_phys = float(config.get('lambda_phys', 0.0)),\n",
        "        phys_num_levels = int(config.get('phys_num_levels', 2)),\n",
        "    )\n",
        "\n",
        "    data_dir = f\"data/{config['reference_frame']}/training/\"\n",
        "    data_dir_validation = f\"data/{config['reference_frame']}/validation/\"\n",
        "\n",
        "    trainer = Trainer(\n",
        "        diffusion,\n",
        "        folder = data_dir,\n",
        "        validation_folder = data_dir_validation,\n",
        "        results_folder = run_dir,\n",
        "        selected_channels = config['selected_channels'],\n",
        "        train_batch_size = config['batch_size'],\n",
        "        test_batch_size = config['batch_size'],\n",
        "        train_lr = config['learning_rate'],\n",
        "        save_and_sample_every = 10000,\n",
        "        train_num_steps = 200000,\n",
        "        ema_decay = 0.995,\n",
        "        log = log,\n",
        "        null_cond_prob = 0.1,\n",
        "        per_frame_cond = config['per_frame_cond'],\n",
        "        reference_frame = config['reference_frame'],\n",
        "        run_name = os.path.basename(os.path.normpath(run_dir)),\n",
        "        accelerator = accelerator,\n",
        "        wandb_username = None,\n",
        "    )\n",
        "\n",
        "    return accelerator, trainer\n",
        "\n",
        "\n",
        "def find_latest_checkpoint(run_dir: str):\n",
        "    import glob\n",
        "    ckpts = sorted(glob.glob(os.path.join(run_dir, 'model', 'step_*', 'checkpoint.pt')))\n",
        "    if not ckpts:\n",
        "        raise FileNotFoundError(f'No checkpoints found under {run_dir}/model/step_*/checkpoint.pt')\n",
        "    # pick highest step\n",
        "    def step_of(p):\n",
        "        m = re.search(r'step_(\\d+)', p)\n",
        "        return int(m.group(1)) if m else -1\n",
        "    ckpts = sorted(ckpts, key=step_of)\n",
        "    return ckpts[-1], step_of(ckpts[-1])\n",
        "\n",
        "\n",
        "# baseline run 目录（来自 pretrained.zip）\n",
        "BASE_RUN_DIR = 'runs/pretrained/'\n",
        "BASE_CONFIG_PATH = os.path.join(BASE_RUN_DIR, 'model', 'model.yaml')\n",
        "base_config = load_yaml(BASE_CONFIG_PATH)\n",
        "print('Loaded base config from:', BASE_CONFIG_PATH)\n",
        "print('base_config keys:', list(base_config.keys()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 4.2) Baseline：加载 checkpoint + 生成 samples ===\n",
        "TARGET_LABELS = 'data/target_responses.csv'\n",
        "GUIDANCE_SCALE = 5.0\n",
        "NUM_PREDS_PER_COND = 1  # 每条目标曲线生成几个样本\n",
        "\n",
        "acc_base, trainer_base = build_model_and_trainer(base_config, BASE_RUN_DIR, log=False, fp16=True)\n",
        "\n",
        "ckpt_path, ckpt_step = find_latest_checkpoint(BASE_RUN_DIR)\n",
        "print('Baseline checkpoint:', ckpt_path, 'step=', ckpt_step)\n",
        "\n",
        "# Trainer.load() 依赖 trainer.step\n",
        "trainer_base.step = ckpt_step\n",
        "trainer_base.load(strict=True)\n",
        "\n",
        "# 生成论文同样的 conditioning samples（保存到 runs/pretrained/eval_target_w_xxx*/step_xxx/）\n",
        "trainer_base.eval_target(TARGET_LABELS, guidance_scale=GUIDANCE_SCALE, num_preds=NUM_PREDS_PER_COND)\n",
        "print('Baseline sampling done.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5) Consistency 实验：相同数据集上 fine-tune（`lambda_phys > 0`）\n",
        "\n",
        "我们在相同训练集 `data/lagrangian/training/` 上，从论文预训练 checkpoint 出发进行短暂 fine-tune，使模型显式最小化你加入的 multi-scale consistency loss。\n",
        "\n",
        "- 你可以把 `LAMBDA_PHYS` 从 `1e-4` / `1e-3` 开始试。\n",
        "- `FINETUNE_STEPS` 默认只跑少量步（Colab 可承受），用于对比趋势；想完全复现实验可把步数加大。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 5.1) 构建 consistency run 并从 baseline checkpoint 开始 fine-tune ===\n",
        "import shutil\n",
        "\n",
        "CONS_RUN_DIR = 'runs/consistency_finetune/'\n",
        "os.makedirs(os.path.join(CONS_RUN_DIR, 'model'), exist_ok=True)\n",
        "os.makedirs(os.path.join(CONS_RUN_DIR, 'training'), exist_ok=True)\n",
        "\n",
        "LAMBDA_PHYS = 1e-4\n",
        "PHYS_LEVELS = 2\n",
        "FINETUNE_STEPS = 5000\n",
        "\n",
        "cons_config = dict(base_config)\n",
        "cons_config['lambda_phys'] = float(LAMBDA_PHYS)\n",
        "cons_config['phys_num_levels'] = int(PHYS_LEVELS)\n",
        "\n",
        "# 保存一份 config，方便复现\n",
        "with open(os.path.join(CONS_RUN_DIR, 'model', 'model.yaml'), 'w') as f:\n",
        "    yaml.safe_dump(cons_config, f)\n",
        "\n",
        "acc_cons, trainer_cons = build_model_and_trainer(cons_config, CONS_RUN_DIR, log=False, fp16=True)\n",
        "\n",
        "# 从 baseline checkpoint 加载\n",
        "trainer_cons.step = ckpt_step\n",
        "trainer_cons.load(strict=True)\n",
        "\n",
        "# 将 train_num_steps 临时设成 finetune 步数（避免跑满 200k）\n",
        "trainer_cons.train_num_steps = trainer_cons.step + FINETUNE_STEPS\n",
        "\n",
        "print('Start finetune: lambda_phys=', LAMBDA_PHYS, 'steps=', FINETUNE_STEPS)\n",
        "trainer_cons.train(load_model_step=ckpt_step, num_samples=1, num_preds=1)\n",
        "print('Finetune done.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 5.2) Consistency 模型：生成同样的 target samples ===\n",
        "# 训练结束后 trainer_cons.step 会停在最后一步\n",
        "cons_step = trainer_cons.step\n",
        "print('Consistency finetuned step:', cons_step)\n",
        "trainer_cons.eval_target(TARGET_LABELS, guidance_scale=GUIDANCE_SCALE, num_preds=NUM_PREDS_PER_COND)\n",
        "print('Consistency sampling done.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6) 对比评估：多尺度自洽性（RG-style）\n",
        "\n",
        "我们对两种模型生成的视频，计算一个简单但直接的“多尺度差异”指标：\n",
        "\n",
        "1. 对生成视频做空间 coarse-graining（2×2 平均池化，重复多次）得到不同尺度的视频；\n",
        "2. 每个尺度上取空间平均得到时序特征 \\(z^{(l)}_t\\in\\mathbb{R}^{C}\\)；\n",
        "3. 计算相邻尺度差异\n",
        "\\[\n",
        "D_l = \\frac{1}{T}\\sum_t \\lVert z^{(l)}_t - z^{(l+1)}_t \\rVert_2.\n",
        "\\]\n",
        "\n",
        "期望：加入一致性 loss 后，\\(D_l\\) 显著更小。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def latest_eval_step_dir(run_dir: str, step: int, guidance_scale: float):\n",
        "    # eval_target 的输出目录形如：runs/<run>/eval_target_w_<w>_<idx>/step_<step>/\n",
        "    pattern = os.path.join(run_dir, f'eval_target_w_{guidance_scale}_*', f'step_{step}')\n",
        "    candidates = sorted(glob.glob(pattern))\n",
        "    if not candidates:\n",
        "        raise FileNotFoundError(f'No eval_target outputs found for pattern: {pattern}')\n",
        "    return candidates[-1]\n",
        "\n",
        "\n",
        "def load_pred_grid_tensor(step_dir: str, pred_channel: int):\n",
        "    # save_preds 存的是一个拼成网格的 GIF，文件名 prediction_channel_<ch>.gif\n",
        "    gif_path = os.path.join(step_dir, 'gifs', f'prediction_channel_{pred_channel}.gif')\n",
        "    from denoising_diffusion_pytorch.video_denoising_diffusion_pytorch import gif_to_tensor\n",
        "    t = gif_to_tensor(gif_path, channels=1)  # (1, f, H, W)\n",
        "    return t\n",
        "\n",
        "\n",
        "def compute_multiscale_D(video: torch.Tensor, levels: int = 2):\n",
        "    \"\"\"video: (c, f, h, w) tensor. Return list [D0, D1, ...].\"\"\"\n",
        "    # reshape to (1,c,f,h,w)\n",
        "    x = video.unsqueeze(0)\n",
        "    pool = torch.nn.AvgPool3d(kernel_size=(1,2,2), stride=(1,2,2))\n",
        "    zs = []\n",
        "    for l in range(levels + 1):\n",
        "        z = x.mean(dim=(-1, -2))  # (1,c,f)\n",
        "        zs.append(z)\n",
        "        if l < levels:\n",
        "            x = pool(x)\n",
        "    Ds = []\n",
        "    for l in range(levels):\n",
        "        d = torch.sqrt(((zs[l] - zs[l+1]) ** 2).mean()).item()\n",
        "        Ds.append(d)\n",
        "    return Ds\n",
        "\n",
        "\n",
        "# 选择一个通道用于一致性评估（建议用位移 u_1=0 或 u_2=1）\n",
        "EVAL_CH = int(base_config['selected_channels'][0])\n",
        "levels = int(cons_config.get('phys_num_levels', 2))\n",
        "\n",
        "base_eval_dir = latest_eval_step_dir(BASE_RUN_DIR, ckpt_step, GUIDANCE_SCALE)\n",
        "cons_eval_dir = latest_eval_step_dir(CONS_RUN_DIR, cons_step, GUIDANCE_SCALE)\n",
        "print('Baseline eval dir:', base_eval_dir)\n",
        "print('Consistency eval dir:', cons_eval_dir)\n",
        "\n",
        "base_vid = load_pred_grid_tensor(base_eval_dir, EVAL_CH)  # (1,f,H,W)\n",
        "cons_vid = load_pred_grid_tensor(cons_eval_dir, EVAL_CH)\n",
        "\n",
        "base_D = compute_multiscale_D(base_vid, levels=levels)\n",
        "cons_D = compute_multiscale_D(cons_vid, levels=levels)\n",
        "\n",
        "print('Baseline D:', base_D)\n",
        "print('Consistency D:', cons_D)\n",
        "\n",
        "# 画柱状图对比\n",
        "x = list(range(levels))\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar([i-0.15 for i in x], base_D, width=0.3, label='baseline (lambda=0)')\n",
        "plt.bar([i+0.15 for i in x], cons_D, width=0.3, label=f'consistency (lambda={LAMBDA_PHYS})')\n",
        "plt.xticks(x, [f'D{i}' for i in x])\n",
        "plt.ylabel('multi-scale difference (lower is better)')\n",
        "plt.title(f'Multi-scale consistency on generated GIF grid (channel {EVAL_CH})')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7) 查看输出文件\n",
        "\n",
        "- Baseline 输出：`runs/pretrained/eval_target_w_<w>_<idx>/step_<step>/gifs/`\n",
        "- Consistency 输出：`runs/consistency_finetune/eval_target_w_<w>_<idx>/step_<step>/gifs/`\n",
        "\n",
        "你可以直接在左侧文件树里点开 GIF；或者用下面的 cell 直接在 notebook 内显示。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "# 展示一个通道的 GIF（网格）\n",
        "base_step_dir = latest_eval_step_dir(BASE_RUN_DIR, ckpt_step, GUIDANCE_SCALE)\n",
        "cons_step_dir = latest_eval_step_dir(CONS_RUN_DIR, cons_step, GUIDANCE_SCALE)\n",
        "\n",
        "base_gif = os.path.join(base_step_dir, 'gifs', f'prediction_channel_{EVAL_CH}.gif')\n",
        "cons_gif = os.path.join(cons_step_dir, 'gifs', f'prediction_channel_{EVAL_CH}.gif')\n",
        "\n",
        "print('Baseline gif:', base_gif)\n",
        "display(Image(filename=base_gif))\n",
        "\n",
        "print('Consistency gif:', cons_gif)\n",
        "display(Image(filename=cons_gif))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
